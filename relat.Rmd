---
title: "O GRANDE TÍTULO"
author: "Alisson Rosa e Vítor Pereira"
abstract: "One Piece > Naruto"
header-includes:
   - \usepackage[brazil]{babel}
   - \usepackage{bm}
geometry: left=1.7cm, right=1.7cm, top=2.5cm, bottom=2.2cm
output:
  bookdown::pdf_document2:
editor_options:
  chunk_output_type: console
indent: true
bibliography: bib.bib 
csl: statistics.csl
nocite: '@*'
link-citations: true

---



```{r setup,include=F}

options(digits = 3) # Arrendodamento
ggplot2::theme_set(ggplot2::theme_minimal()) # Tema dos gráficos produzidos no ggplot2
knitr::opts_chunk$set(echo = F, message = F, warning = F, fig.pos = "H", fig.align = "center", fig.width = 7.8, fig.height = 3.7)
# devtools::install_github("https://github.com/AlissonRP/mypdf1") remova o `#`

library(tidyverse)
library(tidymodels)
library(modeltime)
library(forecast)
# library(workflowsets)
# library(sknifedatar)
```

```{r functions}
barplot <- function(df, v) {
  ggplot(df, aes(
    x = {{ v }},
    y = prop.table(stat(count)),
    fill = {{ v }},
    label = scales::percent(prop.table(stat(count)))
  )) +
    geom_bar(position = "dodge", color = "black") +
    geom_text(
      stat = "count",
      position = position_dodge(.9),
      vjust = -0.5,
      size = 3.5
    ) +
    scale_y_continuous(labels = scales::percent) +
    labs(y = "Proporção", x = df %>%
      select({{ v }}) %>%
      names(), caption = "Fonte: Elaborado pelos autores") +
    theme(plot.title = element_text(hjust = 0.5, size = 10), legend.position = "none")
}

scale_fill_alisson <- function(...) {
  ggplot2:::manual_scale(
    "fill",
    values = setNames(
      c("#EA2129", "#CF6520", "#1EAB78"), levels(as.factor(df$Tipo)),
      ...
    )
  )
}

calcSSE <- function(data, regressor, time, x) {
  loessMod <- try(loess(eval(parse(text = paste(regressor, "~", paste("as.numeric(", time, ")", sep = ""), sep = " "))), data = data, span = x), silent = T)
  res <- try(loessMod$residuals, silent = T)
  if (class(res) != "try-error") {
    sse <- sum(res^2)
  } else {
    sse <- 99999
  }
  return(sse)
}

my_metrics <- default_forecast_accuracy_metric_set()

scale_fill_discrete <- \(...) scale_fill_alisson(...)
```

# Introdução
Naruto é uma animação japonesa (anime)  que adapta a  série de mangá escrita e ilustrada por Masashi Kishimoto, que conta a história de Naruto Uzumaki, um jovem ninja que constantemente procura por reconhecimento e sonha em se tornar Hokage, o ninja líder de sua vila.  
A história é dividida em duas partes, a primeira parte se passa nos anos da pré-adolescência de Naruto (clássico), e a segunda parte se passa em sua adolescência (Shippuden). Nesse trabalho trataremos sobre o Naruto Shippuden, desenvolvendo-o em uma perpestiva de séries temporais, vamos adotar como variável de interesse  a avaliação dos episódios, assim veremos  pontos importantes da saga ao longo do tempo.   
O Banco de dados utilizado é totalmente original, foi criado fazendo *Web scraping* de dois sites diferentes, para aqueles interessados o banco foi dispobilizado [aqui](https://www.kaggle.com/alisson987/naruto-shippuden-rate).


# Informações
```{r }
df <- read_csv("naruto.csv")
```
Como o anime é uma adaptação, existem  episódios fiéis ao mangá (Manga Canon) e episódios  originais do próprio anime, em outras palavras não seguem o material original, esses episódios são chamados de Fillers, existem também episódios que seguem a trama do mangá mas além disso possuem elementos novos, esses chamados de Mixed Canon.  
Vejamos pelo gráfico a seguir a porcentagem do tipo dos episódios

```{r plttipo}
barplot(df, Tipo)
```
É um fato bastante curioso, Naruto Shippuden possui uma quantidade altíssima de episódios fillers, aproximadamente 41%, note pela tabela  que são somente 31 episódios a mais canônicos.
```{r}
df %>%
  group_by(Tipo) %>%
  count() %>%
  rename(Quantidade = n) %>%
  mypdf1::pdf1_tbl("Valores Absolutos do Tipo de Episódio")
```

Em termos de avaliação ao longo do tempo, podemos utilizar como indíce ordinal o próprio número do episódio já que que é uma função injetora no tempo.

```{r}
df %>%
  ggplot(aes(x = numero_episodio, y = rate)) +
  geom_line() +
  gghighlight::gghighlight(200 < numero_episodio & numero_episodio < 400,
    unhighlighted_params = list(colour = "#CB6007")
  ) +
  labs(x = "Numero do episódio", y = "Avaliação")
```
Há bastante coisa a se notar nesse gráfico, primeiramente pelos episódios 200 e 400, existe um indício de alteração na média das variáveis aleatórias, portanto furando o pressuposto de média constante ao longo do tempo, perto dos episódios finais da saga nota-se  também inúmeras quedas bruscas na avaliação sem um contra-peso de avaliações com notas altas, outro indicio de não estacionariedade.  
É importante tentarmos entender um pouco sobre esses episódios com notas baixas, assim vamos ver esse gráfico pelo tipo de episódio.

```{r}
df %>%
  ggplot(aes(x = numero_episodio, y = rate, color = Tipo, group = 1)) +
  geom_line() +
  labs(x = "Numero do episódio", y = "Avaliação") +
  ggplot2::scale_color_manual(breaks = c(levels(as.factor(df$Tipo))), values = c("#EA2129", "#E08223", "#1CD0AD"))
```
Assim fica fácil ver que os episódios com menor avaliação são em sua maior parte fillers, toda queda brusca de avaliação tem um episódio filler envolvido.

É necessário também avaliar a correlação nas avaliações:
```{r}
## esse pacote aqui tem problemas, forecast::ggAcf(df$rate) não funciona, sou obrigado a dar library

ggAcf(df$rate, lag.max = 20, type = c("correlation")) +
  labs(title = "Função de autocorrelação",x="Defasagem")
arco <- df |>
          group_by(Saga) |>
            summarise(total = n()) |>
              (\(x) x[, 2])() |>
                unlist() |>
                  mean()
```
Note que o gráfico anterior as autocorrelações tendem a ficar significativas até 10 defasagens, pois em média a duração dos arcos é `r round(arco,1)`, assim 10 defasagens significa em geral que:  

* o episódio está tendo influência do final do arco anterior;  
* o episódio está gozando consequências dos episódios iníciais do arco que está situado;   
* o episódio está apanhando desfechos dos episódios pós metade do arco que está situado

Uma abordagem importante também é decompor a série temporal, apriori supõe-se a existência de três elementos, a saber: Tendência, sazonalidade e resíduo. Todos esses serão explorados cuidadosamente nas seções posteriores, assim vamos decompor a série para ter um vislumbre de tais elementos:

```{r}
decompose <- forecast::mstl(df$rate)
decompose |> autoplot()+labs(title="Decomposição",x="Número do episódio")
```
A questão de curiosidade vamos ver se os resíduos seguem uma distribuição normal, para isso utilizamos o teste de Shapiro Wilk que possui como hipótese:
$$
H_0:\text{Os dados seguem uma distribuição Normal}
$$
```{r}
# normal K
norm <- data.frame(decompose)$Remainder |>
  shapiro.test() |>
  (\(x) x$p.value)()
```
Efetuando o teste obtemos um p-valor de `r norm`, assim portanto rejeitando a hipótese $H_0$, porém os resíduos não possuirem distribuição normal não afetará as analises daqui pra frente.


# Testes
Os gráficos de seção anterior evidenciaram a possibilidade da série não ser estacionária, assim faz-se necessário verificar se  vale de fato para o processo estocástico, com isso  vamos precisar aplicar testes de hipóteses para averiguar  algumas propriedades, como existência de tendência e  sazonalidade.

## Tendência 

Tendência refere-se a um algum  comportamento   não - estocástico da série em algum momento do tempo, se tal comportamento só acontece em alguns momentos especifícos do tempo, chamamos de tendência estocástica, do contrário é dita determinística. Vamos começar pelos testes de tendência determinística, que em termos sumarizados possuem como hipótese:
$$
H_0:\text{A série não possui tendência determinística}
$$
```{r}
kendal <- trend::mk.test(df$rate)
wald <- trend::ww.test(df$rate)
stuart <- trend::cs.test(df$rate)
tab1 <- as.data.frame(c("Cox-Stuart", "Wald-Wolfowitz", "Mann-Kendall"))
tab1[, 2] <- c(stuart$p.value, wald$p.value, kendal$p.value)
names(tab1) <- c("Testes", "P-valor")

# Sim eu removi os `**` grrr
```

```{r}
tab1 %>%
  mypdf1::pdf1_tbl("Testes de Tendência Determinística")
```

Conforme mostrado pela Tabela anterior podemos ver que pelos testes de 
Cox-Stuart, Wald-Wolfowitz, Mann-Kendall, temos que todos os p-valores são menores pequenos, então concluímos que existe tendência determinística na série temporal. 


## Sazonalidade
Sazonalidade acontece quando a série possui um comportamento que se repete frequencialmente, vamos nessa subseção testar se existe sazonalidade nas avaliação dos episódios, assim temos como hipótese:
$$
H_0:\text{A série não possui sazonalidade}
$$
Os testes aplicados  foram 
Kruskal-Wallis, Friedman e Autocorrelação em lags Sazonais, assim gerando a seguite tabela:

```{r}
krusk <- seastests::kw(df$rate, freq = 7)
friedman <- seastests::fried(df$rate, freq = 7)
tqs <- seastests::qs(df$rate, freq = 7)
tab2 <- as.data.frame(c("Kruskal-Wallis", "Friedman", "Autocorrelação em lags Sazonais"))
tab2[, 2] <- c(krusk$Pval, friedman$Pval, tqs$Pval)
names(tab2) <- c("Testes", "P-valor")
tab2$`P-valor` <- round(tab2$`P-valor`, 4)
tab2$`P-valor` <- ifelse(tab2$`P-valor` < 0.05, ifelse(tab2$`P-valor` < 0.0001, "<0,0001*", paste0(as.character(tab2$`P-valor`), "*")), tab2$`P-valor`)
```

```{r}
tab2 %>%
  mypdf1::pdf1_tbl("Testes de Sazonalidade")
```

Assim, como os testes obtiveram p-valor>0.5, concluímos portanto que não existem indícios de sazonalidade. 

## Raiz unitária
Para começar tal seção, primeiros vamos definir um passeio aleatório em sua forma simplificada:
$$
Y_t=Y_{t-1}+\epsilon_t
$$
Onde t refere-se aos indíces de ordenação, e $\epsilon$  um termo aleatório, para facilidade vamos assumir que $E(\epsilon_t)=\mu$ e $var(\epsilon_t)=\sigma^2$ $\forall t$, assim subtraindo-se $Y_{t-1}$ em ambos os lados tem-se $$Y_t-Y_{t-1}=\epsilon_t$$ que é um processo estacionário, o processo anterior é dito processo estacionário em diferença, em outros casos também chamado de processo integrado. O exemplo anterior trata-se de um caso mais geral $$Y_t=\phi Y_{t-1}+\epsilon_t$$ onde evidentemente $\phi=1$, é fácil mostrar que se $|\phi|<1$ tem-se um processo estocástico, aqui portanto, estamos interessados nesse caso, para isso utilizaremos  testes de hipótese. 

Fala-se também que o caso anterior na forma simplificada possui raiz unitária, por causa do operador Lag, aqui não definido, porém  pode-se ler sobre em @morettin2018analise

```{r}
kpss <- tseries::kpss.test(df$rate, null = c("Level", "Trend"), lshort = TRUE)
adf <- tseries::adf.test(df$rate)
pp <- tseries::pp.test(df$rate)
tab3 <- as.data.frame(c("Kwiatkowski-Phillips-Schmidt-Shin (KPSS)", "Augmented Dickey-Fuller (ADF)", "Phillips-Perron (PP)"))
tab3[, 2] <- c(kpss$p.value, adf$p.value, pp$p.value)
names(tab3) <- c("Testes", "P-valor")
tab3$`P-valor` <- round(tab3$`P-valor`, 4)
tab3$`P-valor` <- ifelse(tab3$`P-valor` < 0.05, ifelse(tab3$`P-valor` < 0.0001, "<0,0001*", paste0(as.character(tab3$`P-valor`), "*")), tab3$`P-valor`)
```

```{r}
tab3 %>%
  mypdf1::pdf1_tbl("Testes de Tendência Estocástica")
```

Conforme mostrado pela Tabela anterior podemos ver que pelos testes de 
Dickey-Fuller Aumentado, Phillips-Perron, KPSS, com índice de significância de 5%, temos que os p-valores dos testes ADF e PP são menores que 0.05 e do teste KPSS é maior que 0.05, então concluímos que não existe tendência estocástica na série temporal. Pois, nos testes ADF e PP, a hipótese nula é a existência de raiz unitária e no teste de KPSS, a hipótese nula é não existência de raiz unitária.

## Resultados 
Vimos visualmente que a série a partir de um certo começa ter uma descrescimento, assim portanto dando um vislumbre de tendência determinística, aplicando os testes obtemos mais uma evidência de existência de tendência na série. Em relação a sazonalidade os testes também confirmaram o que os gráficos mostraram: não existe evidência de sazonalidade na série.

# Modelagem






## Médias Móveis Simples


```{r}
mms <- TTR::SMA(df$rate, n = 10)

mms <- TTR::SMA(df$rate, n = 21)

df1 <- cbind(df, mms)
```


```{r}
df1 %>%
  ggplot(aes(x = numero_episodio)) +
  geom_line(aes(y = rate, colour = "Serie Original")) +
  geom_line(aes(y = mms, colour = "Média Móvel")) +
  labs(x = "Numero do episódio", y = "Avaliação") +
  scale_colour_manual("",
    breaks = c("Serie Original", "Média Móvel"),
    values = c(
      "Serie Original" = "#1EAB78",
      "Média Móvel" = "#CF6520"
    )
  )
```

### Previsão
Utilizando o pacote smooth.

```{r, results = F, fig.show='hide'}
mms1 <- smooth::sma(df$rate[1:400], order = 21, silent = FALSE, interval = "p", h = 100)
fit_forecast <- data.frame(forecast = c(mms1$fitted, mms1$forecast), n = 1:length(c(mms1$fitted, mms1$forecast)))
fit_forecast <- fit_forecast %>%
  mutate(Dados = case_when(
    n == 1:length(mms1$fitted) ~ "Predição",
    n == (length(mms1$fitted) + 1):length(c(mms1$fitted, mms1$forecast)) ~ "Previsão"
  ))
ICbands <- data.frame(upper = mms1$upper, lower = mms1$lower, n = (length(mms1$fitted) + 1):(length(mms1$fitted) + length(mms1$forecast)))
```


```{r}
ggplot() +
  geom_ribbon(data = ICbands, aes(x = n, ymin = lower, ymax = upper), fill = "grey70") +
  geom_line(data = df1, aes(x = numero_episodio, y = rate, colour = "SerieNormal")) +
  geom_line(data = fit_forecast, aes(x = n, y = forecast, colour = "Forecast")) +
  geom_vline(xintercept = max(df1$numero_episodio))
```

```{r}
ggplot() +
  geom_ribbon(data = ICbands, aes(x = n, ymin = lower, ymax = upper), fill = "grey70") +
  geom_line(data = df1, aes(x = numero_episodio, y = rate, colour = "SerieNormal")) +
  geom_line(data = fit_forecast, aes(x = n, y = forecast, colour = "Forecast")) +
  geom_vline(xintercept = max(df1$numero_episodio))
```

## Regressão LOESS

```{r, results = F}
op1 <- optim(par = c(0.5), fn = calcSSE, method = "SANN", data = df1, regressor = "rate", time = "Time")
rl1 <- loess(rate ~ as.numeric(Time), data = df1, span = op1$par)
predict(rl1, seq(as.Date("2017-03-23"), length = 5, by = "week"))
prl <- predict(rl1, as.numeric(df1$Time), se = T)
df1 <- cbind(df, prl$fit) # Página 78, linha 45 à 52.
```


```{r}
df1 %>%
  ggplot(aes(x = numero_episodio)) +
  geom_line(aes(y = rate, colour = "SerieNormal")) +
  geom_line(aes(y = `prl$fit`, colour = "RegLoess")) +
  geom_ribbon(aes(ymin = prl$fit - prl$se.fit, ymax = prl$fit + prl$se.fit, alpha = 0.05)) +
  labs(x = "Numero do episódio", y = "Avaliação") +
  scale_colour_manual("",
    breaks = c("SerieNormal", "RegLoess"),
    values = c(
      "SerieNormal" = "#1EAB78",
      "RegLoess" = "#CF6520"
    )
  )
```

## Suavização Exponencial




### Com outros pacotes

```{r}
dfts <- msts(df %>% select(rate, numero_episodio), seasonal.periods = 365 / 7, start = c(2007, 2.5))
treino <- df[1:400, ]
treino <- window(dfts[, 1], end = c(2014, 38))
teste <- window(dfts[, 1], start = c(2014, 38))

h1 <- holt(treino, h = 100)
h2 <- holt(treino, damped = TRUE, h = 100)

HoltWinters(treino, seasonal = "multiplicative") # Não consegui utilizar voltou o erro de frequência
```
#### Holt não Amortecido

```{r}
forecast_h1 <- data.frame(forecast = c(h1$fitted, h1$mean), n = 1:length(c(h1$fitted, h1$mean)))
ICbands_h1 <- data.frame(upper = h1$upper[, 2], lower = h1$lower[, 2], n = (length(h1$fitted) + 1):(length(h1$fitted) + length(h1$mean)))
forecast_h1 <- forecast_h1 %>%
  mutate(Dados = case_when(
    n == 1:length(h1$fitted) ~ "Predição",
    n == (length(h1$fitted) + 1):length(c(h1$fitted, h1$mean)) ~ "Previsão"
  ))
ggplot() +
  geom_ribbon(data = ICbands_h1, aes(x = n, ymin = lower, ymax = upper), fill = "grey70") +
  geom_line(data = df1, aes(x = numero_episodio, y = rate, colour = "SerieNormal")) +
  geom_line(data = forecast_h1, aes(x = n, y = forecast, colour = "Forecast")) +
  geom_vline(xintercept = max(df1$numero_episodio))
```


#### Holt com Amortecimento

```{r}
forecast_h2 <- data.frame(forecast = c(h2$fitted, h2$mean), n = 1:length(c(h2$fitted, h2$mean)))
ICbands_h2 <- data.frame(upper = h2$upper[, 2], lower = h2$lower[, 2], n = (length(h2$fitted) + 1):(length(h2$fitted) + length(h2$mean)))
forecast_h2 <- forecast_h2 %>%
  mutate(Dados = case_when(
    n == 1:length(h2$fitted) ~ "Predição",
    n == (length(h2$fitted) + 1):length(c(h2$fitted, h2$mean)) ~ "Previsão"
  ))
ggplot() +
  geom_ribbon(data = ICbands_h2, aes(x = n, ymin = lower, ymax = upper), fill = "grey70") +
  geom_line(data = df1, aes(x = numero_episodio, y = rate, colour = "SerieNormal")) +
  geom_line(data = forecast_h2, aes(x = n, y = forecast, colour = "Forecast")) +
  geom_vline(xintercept = max(df1$numero_episodio))
```





## Calculando Métricas de Acurácia dos Modelos não-ModelTime

```{r}
true_values1 <- df %>%
  filter(numero_episodio > max(numero_episodio) - nrow(fit_forecast %>% filter(Dados == "Previsão"))) %>%
  select(rate)
true_values2 <- df %>%
  filter(numero_episodio > max(numero_episodio - nrow(forecast_h2 %>% filter(Dados == "Previsão")))) %>%
  select(rate)
v1 <- fit_forecast %>%
  filter(Dados == "Previsão") %>%
  select(forecast)
v2 <- forecast_h1 %>%
  filter(Dados == "Previsão") %>%
  select(forecast)
v3 <- forecast_h2 %>%
  filter(Dados == "Previsão") %>%
  select(forecast)
dmetrics <- cbind(v1, v2, v3, true_values1, true_values2)
names(dmetrics) <- c("v1", "v2", "v3", "true_values1", "true_values2")
m1 <- my_metrics(dmetrics, true_values1, v1)
m2 <- my_metrics(dmetrics, true_values2, v2)
m3 <- my_metrics(dmetrics, true_values2, v3)
inner_join(m1, m2, by = ".metric") %>%
  inner_join(m3, by = ".metric") -> M1
M1 <- M1 %>% select(-c(.estimator.x, .estimator.y, .estimator))
names(M1) <- c("Métricas", "SMA", "Holt", "Holt amortecido")
Metricas <- unlist(M1 %>% select(Métricas))
M1 <- M1 %>%
  select(-Métricas) %>%
  t() %>%
  as.data.frame()
names(M1) <- str_to_upper(Metricas)
```

```{r}
M1 %>%
  mypdf1::pdf1_tbl("Métricas nos dados de teste")
```
```{r}
df |>
  group_by(Saga) |>
  summarise(total = n()) |>
  (\(x) x[, 2])() |>
  unlist() |>
  mean()
```


# Referências
